# ğŸ” Phi-nder â€” Local RAG System

A fully **offline** Retrieval-Augmented Generation (RAG) application that answers questions from your PDF documents using the **Phi** model via **Ollama**. Built with a beautiful dark-themed **Streamlit** interface.

> **RAG-first strategy** â€” Phi-nder always checks your knowledge base first. It only falls back to the model's own training data when no relevant context is found in your documents.

---

## âœ¨ Features

- ğŸ“„ **PDF Ingestion** â€” Drop PDFs into the `pdfs/` folder and ingest with one click
- ğŸ”¢ **Chunking & Embeddings** â€” Automatic text chunking with overlapping windows + `nomic-embed-text` embeddings via Ollama
- âš¡ **FAISS Vector Search** â€” Lightning-fast similarity search using Facebook's FAISS (L2 distance)
- ğŸ¤– **Phi LLM Answers** â€” Grounded, context-aware answers generated by the Phi model
- ğŸ§  **Model Fallback** â€” Seamlessly falls back to model knowledge when documents don't cover the question
- ğŸ·ï¸ **Source Badges** â€” Clear `ğŸ“š RAG` or `ğŸ§  Model` badges on every answer so you know the source
- ğŸ“‘ **Source Previews** â€” Expandable source chunks with file names for transparency
- ğŸŒŠ **Streaming Responses** â€” Token-by-token streaming for a responsive chat experience
- ğŸ”’ **100% Offline** â€” No data leaves your machine; everything runs locally

---

## ğŸ—ï¸ Architecture

```
PDF Files â”€â”€â†’ Ingestion & Chunking â”€â”€â†’ Embeddings (Ollama) â”€â”€â†’ FAISS Index
                                                                    â”‚
                                                                    â–¼
User Question â”€â”€â†’ Embed Query â”€â”€â†’ FAISS Search â”€â”€â†’ Top-K Chunks â”€â”€â†’ Relevance Check
                                                                        â”‚
                                                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                          â–¼                            â–¼
                                                  Relevant (RAG)              Not Relevant
                                                          â”‚                            â”‚
                                                          â–¼                            â–¼
                                               Phi LLM + Context            Phi LLM (own knowledge)
                                                          â”‚                            â”‚
                                                          â–¼                            â–¼
                                                   ğŸ“š RAG Answer              ğŸ§  Model Answer
```

---

## ğŸ“‹ Prerequisites

1. **Python 3.10+**
2. **Ollama** â€” [Download & Install](https://ollama.com/download)
3. Pull the required models:
   ```bash
   ollama pull phi
   ollama pull nomic-embed-text
   ```

---

## ğŸš€ Quick Start

```bash
# 1. Clone the repository
git clone https://github.com/your-username/Phi-nder.git
cd Phi-nder

# 2. Install dependencies
pip install -r requirements.txt

# 3. Add your PDFs
#    Copy PDF files into the pdfs/ folder

# 4. Make sure Ollama is running
ollama serve

# 5. Launch the app
streamlit run app.py
```

The app will open in your browser at `http://localhost:8501`.

---

## ğŸ“– Usage

1. **Add PDFs** â€” Place your PDF files in the `pdfs/` folder.
2. **Launch** â€” Open the app in your browser (Streamlit will show the URL).
3. **Ingest** â€” Click **ğŸ”„ Ingest PDFs** in the sidebar to build the knowledge base.
4. **Ask** â€” Type your question in the chat input and press Enter.
5. **Review** â€” Check the **source badge** to see if the answer came from your documents or the model's knowledge. Expand **ğŸ“‘ Sources** to see the exact chunks used.

---

## ğŸ“ Project Structure

```
Phi-nder/
â”œâ”€â”€ app.py              # Streamlit UI with dark-themed chat interface
â”œâ”€â”€ config.py           # Central configuration (models, chunking, retrieval)
â”œâ”€â”€ ingestion.py        # PDF loading & sliding-window text chunking
â”œâ”€â”€ embeddings.py       # Ollama embedding generation (single + batch)
â”œâ”€â”€ vector_store.py     # FAISS index build, save, load & search
â”œâ”€â”€ llm.py              # Phi LLM answer generation (RAG + Model modes)
â”œâ”€â”€ rag_pipeline.py     # End-to-end RAG orchestrator with relevance check
â”œâ”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ pdfs/               # Drop your PDF files here
â”‚   â””â”€â”€ .gitkeep
â”œâ”€â”€ faiss_index/        # Auto-generated FAISS index (gitignored)
â””â”€â”€ .gitignore
```

---

## âš™ï¸ Configuration

All settings live in `config.py` and can be overridden via **environment variables**:

| Variable | Default | Description |
|---|---|---|
| `OLLAMA_BASE_URL` | `http://localhost:11434` | Ollama server URL |
| `LLM_MODEL` | `phi` | LLM model for answer generation |
| `EMBED_MODEL` | `nomic-embed-text` | Embedding model for vectorization |
| `CHUNK_SIZE` | `500` | Max characters per text chunk |
| `CHUNK_OVERLAP` | `50` | Overlapping characters between chunks |
| `TOP_K` | `5` | Number of chunks retrieved per query |
| `RELEVANCE_THRESHOLD` | `1.2` | Max L2 distance for a chunk to be considered relevant (0.0â€“2.0) |

### Relevance Threshold Guide

The `RELEVANCE_THRESHOLD` controls when Phi-nder uses RAG vs. model knowledge:

| Range | Behavior |
|---|---|
| `0.5 â€“ 1.0` | **Strict** â€” only highly relevant chunks trigger RAG |
| `1.0 â€“ 1.5` | **Balanced** â€” good relevance with reasonable coverage |
| `1.5 â€“ 1.8` | **Permissive** â€” strongly favors RAG |
| `1.8 â€“ 2.0` | **Very permissive** â€” almost everything uses RAG |

---

## ğŸ§© Dependencies

| Package | Purpose |
|---|---|
| `streamlit` | Web UI framework |
| `PyPDF2` | PDF text extraction |
| `faiss-cpu` | Vector similarity search |
| `numpy` | Numerical operations |
| `requests` | Ollama REST API calls |

Install all at once:

```bash
pip install -r requirements.txt
```

---

## ğŸ’¡ Tips

- **Best PDFs to upload**: Textbooks, research papers, technical docs, manuals, notes â€” anything with extractable text.
- **Scanned PDFs** won't work well since PyPDF2 can't OCR images. Use text-based PDFs for best results.
- **Re-ingest** after adding new PDFs â€” click the sidebar button again to rebuild the index.
- **Tune `RELEVANCE_THRESHOLD`** if you notice too many model fallbacks (increase) or irrelevant RAG answers (decrease).

---

## ğŸ“œ License

This project is open-source. Feel free to use, modify, and distribute.

---

<p align="center">
  <strong>Phi-nder v1.1</strong> Â· Hybrid RAG + Model Â· Fully Offline<br>
  Built with â¤ï¸ using Phi, Ollama, FAISS & Streamlit
</p>
